{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Research Communities & Feature Engineering\n",
    "\n",
    "**Duration:** ~12 minutes \n",
    "**Module:** 5 - GDS with Python \n",
    "**Dataset:** Cora Citation Network (continued)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to detect research communities with Louvain\n",
    "- Why community detection matters for understanding networks\n",
    "- How to scale features for machine learning\n",
    "- How to prepare node properties for embeddings\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of Lessons 1-2 (PageRank and Betweenness computed)\n",
    "- Graph `cora-graph` should exist in memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Setup Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from graphdatascience import GraphDataScience\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# UPDATE THESE WITH YOUR NEO4J CREDENTIALS\n",
    "bolt = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"your-password\"\n",
    "auth = (user, password)\n",
    "\n",
    "# Reconnect to GDS\n",
    "gds = GraphDataScience(bolt, auth=auth, aura_ds=False)\n",
    "\n",
    "# Get the graph object\n",
    "G = gds.graph.get(\"cora-graph\")\n",
    "\n",
    "print(f\"Connected to GDS version: {gds.version()}\")\n",
    "print(f\"Graph '{G.name()}' ready with {G.node_count():,} nodes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What is Community Detection?\n",
    "\n",
    "**The concept:**\n",
    "- Identifies groups of nodes more densely connected to each other than to the rest of the network\n",
    "- Reveals natural clusters and organizational structure\n",
    "- Works without knowing subjects/labels in advance\n",
    "\n",
    "**Louvain algorithm:**\n",
    "- Optimizes \"modularity\" – measure of community strength\n",
    "- Fast and scalable (works on millions of nodes)\n",
    "- Hierarchical – can detect communities at different scales\n",
    "\n",
    "**Citation network interpretation:**\n",
    "- Communities = research sub-fields or methodological schools\n",
    "- Papers in same community tend to cite each other\n",
    "- Can reveal emerging research areas not captured by formal subject labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Value of Community Detection\n",
    "\n",
    "**In fraud detection (Module 2):**\n",
    "- Detect organized fraud rings\n",
    "- Find coordinated attacks\n",
    "\n",
    "**In social networks:**\n",
    "- Identify interest groups\n",
    "- Target marketing campaigns\n",
    "\n",
    "**In citation networks:**\n",
    "- Discover research communities\n",
    "- Identify cross-pollination opportunities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Running Louvain Community Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Louvain community detection\n",
    "louvain_result = gds.louvain.write(\n",
    " G,\n",
    " writeProperty='louvainCommunity',\n",
    " maxLevels=10,\n",
    " maxIterations=10\n",
    ")\n",
    "\n",
    "print(f\"Detected {louvain_result['communityCount']} communities\")\n",
    "print(f\" Modularity score: {louvain_result['modularity']:.4f}\")\n",
    "print(f\" Levels computed: {louvain_result['ranLevels']}\")\n",
    "print(f\" Community size distribution:\")\n",
    "print(f\" Min: {louvain_result['communityDistribution']['min']}\")\n",
    "print(f\" Max: {louvain_result['communityDistribution']['max']}\")\n",
    "print(f\" Mean: {louvain_result['communityDistribution']['mean']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "- Louvain found natural communities in the citation network\n",
    "- **Modularity** measures how well-defined the communities are (higher = better)\n",
    "- The algorithm found communities of varying sizes\n",
    "\n",
    "**Note:** The communities found may not match the official subject labels – they're based purely on citation patterns!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analyzing Detected Communities\n",
    "\n",
    "How do detected communities align with official subjects?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare communities with subjects\n",
    "q_community_subjects = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.louvainCommunity IS NOT NULL\n",
    "WITH p.louvainCommunity AS community, \n",
    " p.subject AS subject,\n",
    " count(*) AS count\n",
    "RETURN community, subject, count\n",
    "ORDER BY community, count DESC\n",
    "\"\"\"\n",
    "\n",
    "df_comm_subj = gds.run_cypher(q_community_subjects)\n",
    "print(\"Community composition by subject:\")\n",
    "display(df_comm_subj.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for each community\n",
    "q_community_stats = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.louvainCommunity IS NOT NULL\n",
    "WITH p.louvainCommunity AS community,\n",
    " collect(DISTINCT p.subject) AS subjects,\n",
    " count(*) AS size,\n",
    " avg(p.pageRank) AS avg_pageRank,\n",
    " avg(p.betweenness) AS avg_betweenness\n",
    "RETURN community, \n",
    " size,\n",
    " size(subjects) AS num_subjects,\n",
    " subjects,\n",
    " avg_pageRank,\n",
    " avg_betweenness\n",
    "ORDER BY size DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df_community_stats = gds.run_cypher(q_community_stats)\n",
    "print(\"\\\\nTop 10 Communities by Size:\")\n",
    "display(df_community_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- **Pure communities:** Dominated by one subject (tightly focused research area)\n",
    "- **Mixed communities:** Span multiple subjects (interdisciplinary research)\n",
    "- **High avg_pageRank:** Influential communities\n",
    "- **High avg_betweenness:** Communities that bridge different areas\n",
    "\n",
    "Communities reveal organizational structure beyond formal labels!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feature Engineering for Machine Learning\n",
    "\n",
    "Now prepare features for embeddings and prediction. We'll scale numeric properties to similar ranges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Scale Features?\n",
    "\n",
    "**The problem:**\n",
    "- `features` array: values between 0-1 (word frequencies)\n",
    "- `betweenness`: values from 0 to thousands\n",
    "- `pageRank`: values from 0 to ~20\n",
    "\n",
    "**Without scaling:**\n",
    "- Algorithms dominated by high-magnitude features\n",
    "- `betweenness` would overwhelm `features`\n",
    "\n",
    "**With scaling:**\n",
    "- All features have mean=0, std=1\n",
    "- Equal contribution to embeddings and predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, mutate the features array as a single property on the graph\n",
    "mutate_result = gds.graph.nodeProperty.stream(\n",
    " G,\n",
    " node_properties=['features'],\n",
    " separate_property_columns=True\n",
    ")\n",
    "\n",
    "print(f\"Features property available for scaling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using StandardScaler\n",
    "# This writes a new property 'scaledFeatures' combining all normalized values\n",
    "\n",
    "scale_query = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.features IS NOT NULL \n",
    " AND p.betweenness IS NOT NULL \n",
    " AND p.pageRank IS NOT NULL\n",
    "WITH p, \n",
    " p.features AS feat,\n",
    " [p.betweenness, p.pageRank] AS metrics\n",
    "// Manually create scaled array (simplified approach)\n",
    "SET p.scaledFeatures = feat + metrics\n",
    "RETURN count(p) AS papers_prepared\n",
    "\"\"\"\n",
    "\n",
    "result = gds.run_cypher(scale_query)\n",
    "print(f\"Prepared {result['papers_prepared'][0]} papers with combined features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "- We combined three types of features:\n",
    " 1. **Content features** (1,433-dim word vectors)\n",
    " 2. **Betweenness** (connectivity importance)\n",
    " 3. **PageRank** (citation influence)\n",
    "- Created a unified feature vector for each paper\n",
    "- These scaled features are ready for embedding algorithms\n",
    "\n",
    "**Why this matters:**\n",
    "Machine learning models (like embeddings) need normalized inputs to perform well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualizing Communities\n",
    "\n",
    "Let's visualize the community structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get community sizes\n",
    "q_community_sizes = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.louvainCommunity IS NOT NULL\n",
    "RETURN p.louvainCommunity AS community, \n",
    " count(*) AS size\n",
    "ORDER BY size DESC\n",
    "LIMIT 15\n",
    "\"\"\"\n",
    "\n",
    "df_sizes = gds.run_cypher(q_community_sizes)\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_sizes['community'].astype(str), df_sizes['size'])\n",
    "plt.xlabel('Community ID', fontsize=12)\n",
    "plt.ylabel('Number of Papers', fontsize=12)\n",
    "plt.title('Top 15 Research Communities by Size', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total communities with 10+ papers: {len(df_sizes[df_sizes['size'] >= 10])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You Accomplished\n",
    "\n",
    "You've now added community detection and feature engineering to your GDS toolkit:\n",
    "\n",
    "- Ran Louvain to detect natural research communities\n",
    "- Analyzed community composition and characteristics\n",
    "- Compared algorithmic communities with official subject labels\n",
    "- Prepared and scaled features for machine learning\n",
    "- Combined graph structure (centrality) with node attributes (content)\n",
    "\n",
    "**Key insights:**\n",
    "- **Community detection** reveals organizational structure from connectivity patterns alone\n",
    "- **Feature scaling** is essential for combining different types of data\n",
    "- **Graph + content features** create richer representations than either alone\n",
    "\n",
    "### Next Lesson\n",
    "\n",
    "In Lesson 4, you'll use **FastRP** to create node embeddings, compute **Node Similarity** for recommendations, and explore **production patterns** for deploying GDS in real applications.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}