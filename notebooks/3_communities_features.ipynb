{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3: Research Communities & Feature Engineering\n",
    "\n",
    "**Duration:** ~12 minutes \n",
    "**Module:** 5 - GDS with Python \n",
    "**Dataset:** Cora Citation Network (continued)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to detect research communities with Louvain\n",
    "- Why community detection matters for understanding networks\n",
    "- How to scale features for machine learning\n",
    "- How to prepare node properties for embeddings\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of Lessons 1-2 (PageRank and Betweenness computed)\n",
    "- Graph `cora-graph` should exist in memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you're re-running this notebook and get an error about the graph `cora-graph-ml` already existing, run this cell to drop it first:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    gds.graph.drop(\"cora-graph-ml\")\n",
    "    print(\"Dropped existing projection\")\n",
    "except:\n",
    "    print(\"No existing projection to drop\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Setup Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from graphdatascience import GraphDataScience\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load credentials from .env\n",
    "load_dotenv()\n",
    "uri = os.getenv('NEO4J_URI')\n",
    "username = os.getenv('NEO4J_USERNAME')\n",
    "password = os.getenv('NEO4J_PASSWORD')\n",
    "\n",
    "# Connect to GDS\n",
    "gds = GraphDataScience(uri, auth=(username, password))\n",
    "\n",
    "# Get the graph object\n",
    "G = gds.graph.get(\"cora-graph\")\n",
    "\n",
    "print(f\"Connected to GDS version: {gds.version()}\")\n",
    "print(f\"Graph '{G.name()}' ready with {G.node_count():,} nodes\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What is Community Detection?\n",
    "\n",
    "**The concept:**\n",
    "- Identifies groups of nodes more densely connected to each other than to the rest of the network\n",
    "- Reveals natural clusters and organizational structure\n",
    "- Works without knowing subjects/labels in advance\n",
    "\n",
    "**Louvain algorithm:**\n",
    "- Optimizes \"modularity\" \u2013 measure of community strength\n",
    "- Fast and scalable (works on millions of nodes)\n",
    "- Hierarchical \u2013 can detect communities at different scales\n",
    "\n",
    "**Citation network interpretation:**\n",
    "- Communities = research sub-fields or methodological schools\n",
    "- Papers in same community tend to cite each other\n",
    "- Can reveal emerging research areas not captured by formal subject labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Value of Community Detection\n",
    "\n",
    "**In fraud detection (Module 2):**\n",
    "- Detect organized fraud rings\n",
    "- Find coordinated attacks\n",
    "\n",
    "**In social networks:**\n",
    "- Identify interest groups\n",
    "- Target marketing campaigns\n",
    "\n",
    "**In citation networks:**\n",
    "- Discover research communities\n",
    "- Identify cross-pollination opportunities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Running Louvain Community Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Louvain community detection\n",
    "louvain_result = gds.louvain.write(\n",
    " G,\n",
    " writeProperty='louvainCommunity',\n",
    " maxLevels=10,\n",
    " maxIterations=10\n",
    ")\n",
    "\n",
    "print(f\"Detected {louvain_result['communityCount']} communities\")\n",
    "print(f\" Modularity score: {louvain_result['modularity']:.4f}\")\n",
    "print(f\" Levels computed: {louvain_result['ranLevels']}\")\n",
    "print(f\" Community size distribution:\")\n",
    "print(f\" Min: {louvain_result['communityDistribution']['min']}\")\n",
    "print(f\" Max: {louvain_result['communityDistribution']['max']}\")\n",
    "print(f\" Mean: {louvain_result['communityDistribution']['mean']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "- Louvain found natural communities in the citation network\n",
    "- **Modularity** measures how well-defined the communities are (higher = better)\n",
    "- The algorithm found communities of varying sizes\n",
    "\n",
    "**Note:** The communities found may not match the official subject labels \u2013 they're based purely on citation patterns!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analyzing Detected Communities\n",
    "\n",
    "How do detected communities align with official subjects?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare communities with subjects\n",
    "q_community_subjects = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.louvainCommunity IS NOT NULL\n",
    "WITH p.louvainCommunity AS community, \n",
    " p.subject AS subject,\n",
    " count(*) AS count\n",
    "RETURN community, subject, count\n",
    "ORDER BY community, count DESC\n",
    "\"\"\"\n",
    "\n",
    "df_comm_subj = gds.run_cypher(q_community_subjects)\n",
    "print(\"Community composition by subject:\")\n",
    "display(df_comm_subj.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for each community\n",
    "q_community_stats = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.louvainCommunity IS NOT NULL\n",
    "WITH p.louvainCommunity AS community,\n",
    " collect(DISTINCT p.subject) AS subjects,\n",
    " count(*) AS size,\n",
    " avg(p.pageRank) AS avg_pageRank,\n",
    " avg(p.betweenness) AS avg_betweenness\n",
    "RETURN community, \n",
    " size,\n",
    " size(subjects) AS num_subjects,\n",
    " subjects,\n",
    " avg_pageRank,\n",
    " avg_betweenness\n",
    "ORDER BY size DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df_community_stats = gds.run_cypher(q_community_stats)\n",
    "print(\"\\\\nTop 10 Communities by Size:\")\n",
    "display(df_community_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- **Pure communities:** Dominated by one subject (tightly focused research area)\n",
    "- **Mixed communities:** Span multiple subjects (interdisciplinary research)\n",
    "- **High avg_pageRank:** Influential communities\n",
    "- **High avg_betweenness:** Communities that bridge different areas\n",
    "\n",
    "Communities reveal organizational structure beyond formal labels!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feature Engineering for Machine Learning\n",
    "\n",
    "Now prepare features for embeddings and prediction. We'll scale numeric properties to similar ranges.\n",
    "\n",
    "First, we need to create a new graph projection that includes the centrality scores we computed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Scale Features?\n",
    "\n",
    "**The problem:**\n",
    "- `features` array: values between 0-1 (word frequencies)\n",
    "- `betweenness`: values from 0 to thousands\n",
    "- `pageRank`: values from 0 to ~20\n",
    "\n",
    "**Without scaling:**\n",
    "- Algorithms dominated by high-magnitude features\n",
    "- `betweenness` would overwhelm `features`\n",
    "\n",
    "**With scaling:**\n",
    "- All features have mean=0, std=1\n",
    "- Equal contribution to embeddings and predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new graph projection that includes all properties we want to use\n",
    "G2, result = gds.graph.project(\n",
    "    'cora-graph-ml',  # New projection for ML\n",
    "    {\n",
    "        'Paper': {\n",
    "            'properties': {\n",
    "                'features': {'property': 'features'},\n",
    "                'betweenness': {'property': 'betweenness', 'defaultValue': 0.0},\n",
    "                'pageRank': {'property': 'pageRank', 'defaultValue': 0.0}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'CITES': {\n",
    "            'orientation': 'UNDIRECTED',\n",
    "            'aggregation': 'SINGLE'\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created projection '{G2.name()}' with {G2.node_count():,} nodes\")\n",
    "print(f\"Available properties: {G2.node_properties('Paper')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all features together using GDS scaleProperties\n",
    "# This combines and normalizes: features (1433-dim) + betweenness + pageRank\n",
    "scaled_result = gds.scaleProperties.mutate(\n",
    "    G2,\n",
    "    nodeProperties=['features', 'betweenness', 'pageRank'],\n",
    "    scaler='MinMax',  # Scale to [0,1] range\n",
    "    mutateProperty='scaledFeatures'\n",
    ")\n",
    "\n",
    "print(f\"Scaled properties for {scaled_result['nodePropertiesWritten']:,} papers\")\n",
    "print(f\"\\nScaler Statistics:\")\n",
    "# Note: min/max are returned as lists (one value per dimension)\n",
    "print(f\"  Betweenness - min: {scaled_result['scalerStatistics']['betweenness']['min'][0]:.2f}, max: {scaled_result['scalerStatistics']['betweenness']['max'][0]:.2f}\")\n",
    "print(f\"  PageRank - min: {scaled_result['scalerStatistics']['pageRank']['min'][0]:.2f}, max: {scaled_result['scalerStatistics']['pageRank']['max'][0]:.2f}\")\n",
    "print(f\"  Features - {len(scaled_result['scalerStatistics']['features']['min'])} dimensions scaled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "- We created a new graph projection (`G2`) including all properties needed for ML\n",
    "- Used `gds.scaleProperties.mutate()` to:\n",
    "  1. Combine three feature types: **content features** (1,433-dim) + **betweenness** + **pageRank**\n",
    "  2. Normalize using MinMax scaling (scales to [0,1] range)\n",
    "  3. Store as a single property: `scaledFeatures`\n",
    "- Now we have a 1,435-dimensional feature vector for each paper\n",
    "\n",
    "**Why scale?**\n",
    "- `features`: values 0-1 (word frequencies)\n",
    "- `betweenness`: values 0-15,000+ (path counts)\n",
    "- `pageRank`: values 0-20+ (influence scores)\n",
    "\n",
    "Without scaling, betweenness would dominate. Scaling ensures equal contribution!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualizing Communities\n",
    "\n",
    "Let's visualize the community structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get community sizes\n",
    "q_community_sizes = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.louvainCommunity IS NOT NULL\n",
    "RETURN p.louvainCommunity AS community, \n",
    " count(*) AS size\n",
    "ORDER BY size DESC\n",
    "LIMIT 15\n",
    "\"\"\"\n",
    "\n",
    "df_sizes = gds.run_cypher(q_community_sizes)\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(df_sizes['community'].astype(str), df_sizes['size'])\n",
    "plt.xlabel('Community ID', fontsize=12)\n",
    "plt.ylabel('Number of Papers', fontsize=12)\n",
    "plt.title('Top 15 Research Communities by Size', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total communities with 10+ papers: {len(df_sizes[df_sizes['size'] >= 10])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You Accomplished\n",
    "\n",
    "You've now added community detection and feature engineering to your GDS toolkit:\n",
    "\n",
    "- Ran Louvain to detect natural research communities\n",
    "- Analyzed community composition and characteristics\n",
    "- Compared algorithmic communities with official subject labels\n",
    "- Prepared and scaled features for machine learning\n",
    "- Combined graph structure (centrality) with node attributes (content)\n",
    "\n",
    "**Key insights:**\n",
    "- **Community detection** reveals organizational structure from connectivity patterns alone\n",
    "- **Feature scaling** is essential for combining different types of data\n",
    "- **Graph + content features** create richer representations than either alone\n",
    "\n",
    "### Next Lesson\n",
    "\n",
    "In Lesson 4, you'll use **FastRP** to create node embeddings, compute **Node Similarity** for recommendations, and explore **production patterns** for deploying GDS in real applications.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}