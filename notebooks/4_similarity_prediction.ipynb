{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Similarity & Prediction\n",
    "\n",
    "**Duration:** ~15 minutes \n",
    "**Module:** 5 - GDS with Python \n",
    "**Dataset:** Cora Citation Network (continued)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to create node embeddings with FastRP\n",
    "- How to compute similarity between papers\n",
    "- How to make link predictions (recommend citations)\n",
    "- Production patterns for GDS workflows\n",
    "- How to clean up graph projections\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completion of Lessons 1-3 (all centrality, communities, and features computed)\n",
    "- Graph `cora-graph` should exist in memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Setup Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from graphdatascience import GraphDataScience\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load credentials from .env\n",
    "load_dotenv()\n",
    "uri = os.getenv('NEO4J_URI')\n",
    "username = os.getenv('NEO4J_USERNAME')\n",
    "password = os.getenv('NEO4J_PASSWORD')\n",
    "\n",
    "# Connect to GDS\n",
    "aura_ds = 'neo4j+s://' in uri if uri else False\n",
    "gds = GraphDataScience(uri, auth=(username, password), aura_ds=aura_ds)\n",
    "\n",
    "# Get the graph object\n",
    "G = gds.graph.get(\"cora-graph\")\n",
    "\n",
    "print(f\"Connected to GDS version: {gds.version()}\")\n",
    "print(f\"Graph '{G.name()}' ready: {G.node_count():,} nodes\")\n",
    "print(f\"Node properties: {G.node_properties('Paper')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What are Node Embeddings?\n",
    "\n",
    "**The concept:**\n",
    "- Transform nodes into dense vector representations (embeddings)\n",
    "- Similar nodes -> similar vectors\n",
    "- Enables machine learning on graphs (clustering, classification, link prediction)\n",
    "\n",
    "**FastRP (Fast Random Projection):**\n",
    "- Creates embeddings from graph structure + node properties\n",
    "- Much faster than deep learning approaches (e.g., GraphSAGE)\n",
    "- Works well on large graphs (millions of nodes)\n",
    "- Captures both local and global graph structure\n",
    "\n",
    "**What influences the embedding?**\n",
    "1. **Graph structure:** Papers citing similar papers get similar embeddings\n",
    "2. **Node features:** Papers with similar content get similar embeddings\n",
    "3. **Centrality:** We included PageRank & Betweenness in our features!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Value of Embeddings\n",
    "\n",
    "**In e-commerce:**\n",
    "- Product recommendations based on user behavior graph\n",
    "\n",
    "**In fraud detection:**\n",
    "- Classify suspicious accounts based on transaction patterns\n",
    "\n",
    "**In citation networks:**\n",
    "- Recommend papers for researchers to read\n",
    "- Predict future citations (link prediction)\n",
    "- Cluster papers into research topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating Embeddings with FastRP\n",
    "\n",
    "Run FastRP using the scaled features from Lesson 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FastRP to create 128-dimensional embeddings\n",
    "# Use MUTATE mode to keep embeddings in memory (not written to database yet)\n",
    "\n",
    "fastrp_result = gds.fastRP.mutate(\n",
    " G,\n",
    " mutateProperty='embedding',\n",
    " embeddingDimension=128,\n",
    " featureProperties=['scaledFeatures'], # Use our prepared features\n",
    " randomSeed=42, # For reproducibility\n",
    " iterationWeights=[0.0, 1.0, 1.0] # Weight more recent iterations\n",
    ")\n",
    "\n",
    "print(f\"Created {fastrp_result['nodePropertiesWritten']} embeddings\")\n",
    "print(f\" Embedding dimension: {embeddingDimension}\")\n",
    "print(f\" Based on {len(['scaledFeatures'])} feature properties\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "- FastRP created a 128-dimensional vector for each paper\n",
    "- Used **MUTATE** mode (keeps embeddings in the projection, doesn't write to DB)\n",
    "- Combined graph structure with content features\n",
    "- `iterationWeights` controls how much to weight different \"hops\" in the graph\n",
    "\n",
    "**Why MUTATE instead of WRITE?**\n",
    "- Faster (no database write)\n",
    "- Useful for intermediate results\n",
    "- Can still stream/write later if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Computing Node Similarity\n",
    "\n",
    "Now use the embeddings to find similar papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Node Similarity to find the top-5 most similar papers for each paper\n",
    "# Use STREAM mode to get results directly without writing\n",
    "\n",
    "similarity_stream = gds.nodeSimilarity.stream(\n",
    " G,\n",
    " nodeProperties=['embedding'],\n",
    " topK=5, # Top 5 most similar papers per paper\n",
    " similarityCutoff=0.5 # Only pairs with similarity > 0.5\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_similarity = pd.DataFrame(similarity_stream)\n",
    "\n",
    "print(f\"Computed {len(df_similarity):,} similarity pairs\")\n",
    "print(f\" Average similarity: {df_similarity['similarity'].mean():.3f}\")\n",
    "print(f\" Max similarity: {df_similarity['similarity'].max():.3f}\")\n",
    "print(\"\\nSample similarity pairs:\")\n",
    "display(df_similarity.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the results:**\n",
    "- `node1`, `node2`: Neo4j internal IDs\n",
    "- `similarity`: Cosine similarity (0-1, higher = more similar)\n",
    "- Each paper gets top-5 recommendations\n",
    "- Only pairs above 0.5 similarity are returned\n",
    "\n",
    "**This is the foundation of recommendation systems!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Paper Recommendations\n",
    "\n",
    "Let's make this concrete by recommending papers for a specific paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a high-PageRank paper and find similar papers\n",
    "q_recommendations = \"\"\"\n",
    "MATCH (source:Paper)\n",
    "WHERE source.pageRank > 10\n",
    "WITH source\n",
    "ORDER BY source.pageRank DESC\n",
    "LIMIT 1\n",
    "\n",
    "// Get source paper details\n",
    "WITH source, \n",
    " id(source) AS sourceId,\n",
    " source.paper_Id AS source_paperId,\n",
    " source.subject AS source_subject,\n",
    " source.pageRank AS source_pageRank\n",
    "\n",
    "// Find papers similar to this source (using pre-computed similarity)\n",
    "MATCH (source)-[:SIMILAR]-(similar:Paper)\n",
    "WHERE similar.paper_Id <> source.paper_Id\n",
    "\n",
    "RETURN \n",
    " source_paperId,\n",
    " source_subject,\n",
    " source_pageRank,\n",
    " collect({\n",
    " paperId: similar.paper_Id,\n",
    " subject: similar.subject,\n",
    " pageRank: similar.pageRank\n",
    " })[..5] AS recommendations\n",
    "\"\"\"\n",
    "\n",
    "# Note: This query assumes SIMILAR relationships were written\n",
    "# For this demo, let's query differently using the stream results\n",
    "\n",
    "# Get a specific paper and its neighbors\n",
    "sample_paper_id = 100\n",
    "\n",
    "q_recommend = f\"\"\"\n",
    "MATCH (p:Paper {{paper_Id: {sample_paper_id}}})\n",
    "RETURN \n",
    " p.paper_Id AS paperId,\n",
    " p.subject AS subject,\n",
    " p.pageRank AS pageRank,\n",
    " p.betweenness AS betweenness,\n",
    " p.louvainCommunity AS community\n",
    "\"\"\"\n",
    "\n",
    "source_paper = gds.run_cypher(q_recommend)\n",
    "print(\"Source Paper:\")\n",
    "display(source_paper)\n",
    "\n",
    "# Find similar papers from our similarity stream\n",
    "# (In production, you'd write these as relationships)\n",
    "print(f\"\\\\nTop 5 Papers Most Similar to Paper {sample_paper_id}:\")\n",
    "print(\"(Based on combined structure + content + centrality)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "\n",
    "Similar papers might:\n",
    "1. **Same subject** -> Related research\n",
    "2. **Different subject** -> Interdisciplinary connections\n",
    "3. **Similar centrality** -> Papers with similar structural importance\n",
    "\n",
    "This is more sophisticated than just \"papers in the same category\"!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Clustering with K-Means\n",
    "\n",
    "Use the embeddings to cluster papers into research topics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-Means clustering on the embeddings\n",
    "# We know there are 7 official subjects, so let's try k=7\n",
    "\n",
    "kmeans_result = gds.kmeans.write(\n",
    " G,\n",
    " nodeProperty='embedding',\n",
    " k=7, # 7 clusters (same as number of subjects)\n",
    " writeProperty='cluster',\n",
    " randomSeed=42\n",
    ")\n",
    "\n",
    "print(f\"Created {kmeans_result['communityDistribution']['max'] + 1} clusters\")\n",
    "print(f\" Silhouette score: {kmeans_result.get('silhouette', 'N/A')}\")\n",
    "print(f\" Cluster sizes: min={kmeans_result['communityDistribution']['min']}, max={kmeans_result['communityDistribution']['max']}, mean={kmeans_result['communityDistribution']['mean']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare K-Means clusters with official subjects\n",
    "q_cluster_analysis = \"\"\"\n",
    "MATCH (p:Paper)\n",
    "WHERE p.cluster IS NOT NULL\n",
    "WITH p.cluster AS cluster,\n",
    " p.subject AS subject,\n",
    " count(*) AS count\n",
    "RETURN cluster, subject, count\n",
    "ORDER BY cluster, count DESC\n",
    "\"\"\"\n",
    "\n",
    "df_clusters = gds.run_cypher(q_cluster_analysis)\n",
    "print(\"\\nK-Means Cluster Composition:\")\n",
    "display(df_clusters.head(25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight:**\n",
    "\n",
    "K-Means clusters papers based on:\n",
    "- **Graph structure** (citation patterns)\n",
    "- **Content similarity** (word features) \n",
    "- **Centrality** (PageRank + Betweenness)\n",
    "\n",
    "Compare with **Louvain** from Lesson 3:\n",
    "- **Louvain:** Pure graph structure (modularity)\n",
    "- **K-Means on embeddings:** Structure + content + centrality\n",
    "\n",
    "Different algorithms -> different perspectives on the same data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Production Patterns\n",
    "\n",
    "Best practices for using GDS in real applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 1: Estimate Memory Before Running\n",
    "\n",
    "Always estimate memory for large graphs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate memory for PageRank on a large graph\n",
    "estimate = gds.pageRank.write.estimate(\n",
    " G,\n",
    " writeProperty='pageRank_test'\n",
    ")\n",
    "\n",
    "print(\"PageRank Memory Estimate:\")\n",
    "print(f\" Min bytes: {estimate['minMemoryUsage']}\")\n",
    "print(f\" Max bytes: {estimate['maxMemoryUsage']}\")\n",
    "print(f\" Node count: {estimate['nodeCount']:,}\")\n",
    "print(f\" Relationship count: {estimate['relationshipCount']:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Use STREAM for Exploration, WRITE for Production\n",
    "\n",
    "**STREAM mode:**\n",
    "- Returns results as pandas DataFrame\n",
    "- No database writes\n",
    "- Good for exploration and testing\n",
    "\n",
    "**MUTATE mode:**\n",
    "- Stores results in projection (not database)\n",
    "- Good for intermediate steps in pipeline\n",
    "\n",
    "**WRITE mode:**\n",
    "- Stores results as node/relationship properties\n",
    "- Persists for future queries\n",
    "- Good for production\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: List and Clean Up Projections\n",
    "\n",
    "Always clean up projections when done!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all graph projections\n",
    "projections = gds.graph.list()\n",
    "print(\"Current graph projections:\")\n",
    "display(projections[['graphName', 'nodeCount', 'relationshipCount', 'memoryUsage']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To drop a projection when you're done:\n",
    "# (Uncomment to run)\n",
    "\n",
    "# gds.graph.drop(G)\n",
    "# print(\"Dropped projection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 4: Batch Processing Pipeline\n",
    "\n",
    "Example production workflow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citation_analysis_pipeline(gds_client, graph_name=\"citation-pipeline\"):\n",
    " \"\"\"\n",
    " Complete citation network analysis pipeline.\n",
    " \"\"\"\n",
    " print(\"Step 1: Project graph\")\n",
    " G, _ = gds_client.graph.project(\n",
    " graph_name,\n",
    " {\"Paper\": {\"properties\": [\"features\", \"subjectClass\"]}},\n",
    " {\"CITES\": {\"orientation\": \"UNDIRECTED\"}}\n",
    " )\n",
    " \n",
    " print(\"Step 2: Compute centrality metrics\")\n",
    " gds_client.pageRank.mutate(G, mutateProperty='pr')\n",
    " gds_client.betweenness.mutate(G, mutateProperty='bc')\n",
    " \n",
    " print(\"Step 3: Detect communities\")\n",
    " gds_client.louvain.mutate(G, mutateProperty='community')\n",
    " \n",
    " print(\"Step 4: Create embeddings\")\n",
    " gds_client.fastRP.mutate(\n",
    " G,\n",
    " mutateProperty='emb',\n",
    " embeddingDimension=128,\n",
    " featureProperties=['features']\n",
    " )\n",
    " \n",
    " print(\"Step 5: Compute similarity\")\n",
    " similarity = gds_client.nodeSimilarity.stream(\n",
    " G,\n",
    " nodeProperties=['emb'],\n",
    " topK=10\n",
    " )\n",
    " \n",
    " print(\"Step 6: Clean up\")\n",
    " gds_client.graph.drop(G)\n",
    " \n",
    " return pd.DataFrame(similarity)\n",
    "\n",
    "# This pattern is perfect for scheduled batch jobs!\n",
    "print(\"\\nProduction Pipeline Template Created \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You Accomplished\n",
    "\n",
    "You've completed the entire Python GDS workflow!\n",
    "\n",
    "- Created node embeddings with FastRP (combining structure + content)\n",
    "- Computed node similarity for recommendations\n",
    "- Clustered papers with K-Means\n",
    "- Learned production patterns (estimate, stream/mutate/write, cleanup)\n",
    "- Built a complete analysis pipeline\n",
    "\n",
    "**Module 5 Complete!**\n",
    "\n",
    "### What You've Learned Across All Lessons\n",
    "\n",
    "**Lesson 1:** Setup, data loading, projections, PageRank \n",
    "**Lesson 2:** Betweenness Centrality, bridge analysis \n",
    "**Lesson 3:** Louvain communities, feature engineering \n",
    "**Lesson 4:** FastRP embeddings, similarity, K-Means, production patterns\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Python GDS client** simplifies graph data science workflows\n",
    "2. **Combine multiple algorithms** for richer insights\n",
    "3. **Structure + Content + Centrality** -> powerful embeddings\n",
    "4. **Production patterns** ensure scalable, maintainable code\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Apply these techniques to your own datasets\n",
    "- Explore other GDS algorithms (see GDS documentation)\n",
    "- Build recommendation systems, fraud detection, knowledge graphs\n",
    "- Consider **Aura Graph Analytics** for on-demand scalable GDS\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
